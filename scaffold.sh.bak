```bash
#!/usr/bin/env bash
set -euo pipefail

################################################################################
# 1. Directory scaffolding
################################################################################
mkdir -p ragstream/{app,config,ingestion,retrieval,orchestration,tooling,utils}
mkdir -p data/{doc_raw,chroma_db,vector_pkls}
mkdir -p logs

# ensure each package is import-ready
touch ragstream/__init__.py \
      ragstream/app/__init__.py \
      ragstream/config/__init__.py \
      ragstream/ingestion/__init__.py \
      ragstream/retrieval/__init__.py \
      ragstream/orchestration/__init__.py \
      ragstream/tooling/__init__.py \
      ragstream/utils/__init__.py

################################################################################
# 2. Utils
################################################################################
cat > ragstream/utils/paths.py <<'PY'
"""
Paths
=====
Centralises all on-disk locations for the project so that a single import
(`from ragstream.utils.paths import PATHS`) provides **typed** access to
directories used across ingestion, vector store, caching, and logging.
"""
from pathlib import Path
from typing import TypedDict

class _Paths(TypedDict):
    root:        Path
    data:        Path
    raw_docs:    Path
    chroma_db:   Path
    vector_pkls: Path
    logs:        Path

ROOT = Path(__file__).resolve().parents[2]

PATHS: _Paths = {
    "root":        ROOT,
    "data":        ROOT / "data",
    "raw_docs":    ROOT / "data" / "doc_raw",
    "chroma_db":   ROOT / "data" / "chroma_db",   # planned
    "vector_pkls": ROOT / "data" / "vector_pkls", # current persistence
    "logs":        ROOT / "logs",
}
PY

cat > ragstream/utils/logging.py <<'PY'
"""
SimpleLogger
============
Ultra-light wrapper around the built-in *logging* module so that the whole
code-base can depend on **one** unified logger without external config files.
"""
import logging

class SimpleLogger:
    """Minimal façade for standard logging (single responsibility: output)."""
    _logger = logging.getLogger("ragstream")
    if not _logger.handlers:
        _logger.setLevel(logging.INFO)
        _handler = logging.StreamHandler()
        _formatter = logging.Formatter("[%(asctime)s] %(levelname)s : %(message)s")
        _handler.setFormatter(_formatter)
        _logger.addHandler(_handler)

    @classmethod
    def log(cls, msg: str) -> None:
        """Write an *INFO* message (human-friendly)."""
        cls._logger.info(msg)

    @classmethod
    def error(cls, msg: str) -> None:
        """Write an *ERROR* message (something went wrong)."""
        cls._logger.error(msg)
PY

################################################################################
# 3. Config
################################################################################
cat > ragstream/config/settings.py <<'PY'
"""
Settings
========
Loads environment variables **once** at start-up so that every module accesses
configuration via Settings.get("OPENAI_API_KEY") instead of scattered os.getenv.
"""
import os
from typing import Any, Dict

class Settings:
    """
    Thin, immutable wrapper around `os.environ`.

    *Purpose*: centralise **all** config keys (API keys, flags, paths) and offer
    a single interface for default values & type-checking.
    """
    _CACHE: Dict[str, Any] = {}

    @classmethod
    def get(cls, key: str, default: Any | None = None) -> Any:
        """Return env value or *default*; cache the lookup."""
        if key not in cls._CACHE:
            cls._CACHE[key] = os.getenv(key, default)
        return cls._CACHE[key]
PY

################################################################################
# 4. Ingestion layer
# NOTE: per instruction, DO NOT rewrite anything under ragstream/ingestion.
################################################################################
cat > ragstream/ingestion/loader.py <<'PY'
"""
DocumentLoader
==============
Responsible for discovering and loading raw files from *data/doc_raw*.
"""
from pathlib import Path
from typing import List

class DocumentLoader:
    """Scans the raw-document directory and yields file paths."""
    def __init__(self, root: Path) -> None:
        self.root = root

    def load_documents(self) -> List[str]:
        """Return a list of file paths (as strings); real parsing is deferred."""
        return []
PY

cat > ragstream/ingestion/chunker.py <<'PY'
"""
Chunker
=======
Splits raw text into overlapping, token-aware chunks for embedding.
"""
from typing import List

class Chunker:
    """Window-based text splitter."""
    def split(self, text: str) -> List[str]:
        """Return list of overlapping chunks."""
        return []
PY

cat > ragstream/ingestion/embedder.py <<'PY'
"""
Embedder
========
Wraps SentenceTransformers to convert text chunks into dense vectors.
"""
from typing import List

class Embedder:
    """High-level embedding interface."""
    def embed(self, texts: List[str]) -> List[list[float]]:
        """Return list of embedding vectors (dummy)."""
        return []
PY

cat > ragstream/ingestion/vector_store.py <<'PY'
"""
VectorStore
===========
Thin façade around Chroma (or any future DB) handling persistence & snapshots.
"""
from typing import List

class VectorStore:
    """Stores embeddings and metadata; persists to disk."""
    def add(self, ids: List[str], vectors: List[list[float]], meta: List[dict]) -> None:
        """Insert or update vectors; no implementation yet."""
        return

    def query(self, vector: list[float], k: int = 10) -> List[str]:
        """Return top-k IDs (dummy)."""
        return []

    def snapshot(self, timestamp: str) -> None:
        """Create a timestamped backup of the DB on disk."""
        return
PY

################################################################################
# 5. Retrieval layer
################################################################################
cat > ragstream/retrieval/attention.py <<'PY'
"""
AttentionWeights (legacy shim)
==============================
Kept for legacy tests. In the current design, eligibility is controlled via
ON/OFF per-file toggles and Exact File Lock in the UI, not sliders.
"""
from typing import Dict

class AttentionWeights:
    """No-op weight application kept for backward compatibility."""
    def weight(self, scores: Dict[str, float]) -> Dict[str, float]:
        return scores
PY

cat > ragstream/retrieval/reranker.py <<'PY'
"""
Reranker
========
Cross-encoder that re-orders the initially retrieved chunks.
"""
from typing import List

class Reranker:
    """Cross-encoder interface."""
    def rerank(self, ids: List[str], query: str) -> List[str]:
        """Return list of IDs sorted by relevance (dummy)."""
        return ids
PY

cat > ragstream/retrieval/retriever.py <<'PY'
"""
Retriever
=========
Coordinates vector search and reranking. In the new design, per-file eligibility
is handled in the controller (Exact File Lock / ON-OFF pool) before retrieval.
"""
from typing import List

class DocScore:
    """Lightweight value object pairing *doc_id* with *score*."""
    def __init__(self, doc_id: str, score: float) -> None:
        self.id = doc_id
        self.score = score

class Retriever:
    """End-to-end retrieval pipeline orchestrator."""
    def retrieve(self, query: str, k: int = 10) -> List[DocScore]:
        """Return reranked `DocScore` list (dummy)."""
        return []
PY

################################################################################
# 6. Orchestration layer
################################################################################
cat > ragstream/orchestration/prompt_builder.py <<'PY'
"""
PromptBuilder
=============
Assembles the final Super-Prompt from:
  - ❖ FILES (deterministic block by A1)
  - S_ctx (Facts / Constraints / Open Issues by A4)
  - tool_output (if any)
and applies the fixed authority order:
[Hard Rules] → [Project Memory] → [❖ FILES] → [S_ctx] → [Task/Mode]
"""
from typing import List, Optional

class PromptBuilder:
    """Template-driven prompt composer."""
    def build(self, question: str, s_ctx: List[str], files_block: Optional[str] = None, tool: Optional[str] = None) -> str:
        """Return composed prompt (dummy)."""
        return "PROMPT"
PY

cat > ragstream/orchestration/llm_client.py <<'PY'
"""
LLMClient
=========
Thin adapter around OpenAI (or local model) providing streaming & retry.
Also exposes a basic cost estimator for UI display.
"""
class LLMClient:
    """Handles completion calls and cost estimation."""
    def complete(self, prompt: str) -> str:
        """Return LLM answer (dummy)."""
        return "ANSWER"

    def estimate_cost(self, tokens: int) -> float:
        """Rough USD cost estimate based on model pricing."""
        return 0.0
PY

################################################################################
# 7. Tooling layer
################################################################################
cat > ragstream/tooling/base_tool.py <<'PY'
"""
BaseTool
========
Abstract base class for any local executable helper (math, python, shell …).
"""
class BaseTool:
    """Every concrete tool must implement `__call__`."""
    name: str = "base"

    def __call__(self, instruction: str) -> str:
        """Execute the tool and return textual output (dummy)."""
        return "<tool-output>"
PY

cat > ragstream/tooling/math_tool.py <<'PY'
"""
MathTool
========
Evaluates arithmetic expressions (safe subset) and returns the result.
"""
from ragstream.tooling.base_tool import BaseTool

class MathTool(BaseTool):
    """Protected SymPy evaluator."""
    name = "math"

    def __call__(self, instruction: str) -> str:
        """Parse and compute math expression (dummy)."""
        return "0"
PY

cat > ragstream/tooling/py_tool.py <<'PY'
"""
PyTool
======
Executes short Python snippets inside a restricted sandbox.
"""
from ragstream.tooling.base_tool import BaseTool

class PyTool(BaseTool):
    """RestrictedPython sandbox executor."""
    name = "py"

    def __call__(self, instruction: str) -> str:
        """Execute code and capture stdout (dummy)."""
        return "<py-result>"
PY

cat > ragstream/tooling/registry.py <<'PY'
"""
ToolRegistry
============
Discovers all subclasses of BaseTool and exposes them via .get(name).
"""
from typing import Dict
from ragstream.tooling.base_tool import BaseTool

class ToolRegistry:
    """Keeps a mapping `name -> tool_instance`."""
    _registry: Dict[str, BaseTool] = {}

    @classmethod
    def discover(cls) -> None:
        """Populate registry (dummy)."""
        return

    @classmethod
    def get(cls, name: str) -> BaseTool:
        """Return tool instance or raise KeyError."""
        return cls._registry[name]
PY

cat > ragstream/tooling/dispatcher.py <<'PY'
"""
ToolDispatcher
==============
Detects `calc:` / `py:` prefixes in the user prompt and routes to the tool.
"""
from typing import Tuple

class ToolDispatcher:
    """Front controller for local tool execution."""
    def maybe_dispatch(self, prompt: str) -> Tuple[str, str]:
        """
        Returns (tool_output, stripped_prompt).
        If no tool prefix detected, tool_output = "".
        """
        return ("", prompt)
PY

################################################################################
# 8. Application / UI
################################################################################
cat > ragstream/app/agents.py <<'PY'
"""
Agents (A1..A4)
===============
Controller-side agents:
- A1 Deterministic Code Injector → builds ❖ FILES (FULL/PACK) and enforces Exact File Lock.
- A2 Prompt Shaper → suggests intent/domain + headers (advisory).
- A3 NLI Gate → keep/drop based on entailment with θ strictness.
- A4 Context Condenser → outputs S_ctx (Facts / Constraints / Open Issues) with citations.
"""
from typing import List, Dict, Tuple, Optional

class A1_DCI:
    def build_files_block(self, named_files: List[str], lock: bool) -> str:
        return "❖ FILES\n"

class A2_PromptShaper:
    def propose(self, question: str) -> Dict[str, str]:
        return {"intent": "explain", "domain": "software"}

class A3_NLIGate:
    def __init__(self, theta: float = 0.6) -> None:
        self.theta = theta
    def filter(self, candidates: List[str], question: str) -> List[str]:
        return candidates

class A4_Condenser:
    def condense(self, kept: List[str]) -> List[str]:
        return ["Facts:", "Constraints:", "Open Issues:"]
PY

cat > ragstream/app/controller.py <<'PY'
"""
AppController
=============
Glue code between Streamlit UI, eligibility controls (ON/OFF + Exact File Lock),
retrieval pipeline, agents A1..A4, tooling dispatcher, and LLM client.
"""
from typing import List, Tuple
from ragstream.retrieval.retriever import Retriever, DocScore
from ragstream.tooling.dispatcher import ToolDispatcher
from ragstream.orchestration.prompt_builder import PromptBuilder
from ragstream.orchestration.llm_client import LLMClient
from ragstream.app.agents import A1_DCI, A2_PromptShaper, A3_NLIGate, A4_Condenser

class AppController:
    """High-level façade used by the UI callbacks."""
    def __init__(self) -> None:
        self.retriever = Retriever()
        self.dispatcher = ToolDispatcher()
        self.prompt_builder = PromptBuilder()
        self.llm_client = LLMClient()
        # Agents
        self.a1 = A1_DCI()
        self.a2 = A2_PromptShaper()
        self.a3 = A3_NLIGate()
        self.a4 = A4_Condenser()

    def run_query(self, question: str, named_files: List[str], exact_file_lock: bool) -> str:
        """Full workflow returning final answer (dummy)."""
        # A2 propose headers (advisory)
        _headers = self.a2.propose(question)
        # A1 files block
        files_block = self.a1.build_files_block(named_files, exact_file_lock)
        # Retrieval path (skipped if lock)
        if exact_file_lock:
            kept_chunks: List[str] = []
        else:
            # retrieve → A3 gate
            results: List[DocScore] = self.retriever.retrieve(question, k=10)
            ids = [r.id for r in results]
            kept_chunks = self.a3.filter(ids, question)
        # A4 condense
        s_ctx = self.a4.condense(kept_chunks)
        # Prompt
        prompt = self.prompt_builder.build(question, s_ctx, files_block, tool=None)
        # LLM
        return self.llm_client.complete(prompt)
PY

cat > ragstream/app/ui_streamlit.py <<'PY'
"""
StreamlitUI
===========
Defines the visible Streamlit components (prompt box, file ON/OFF controls,
Exact File Lock toggle, Prompt Shaper panel, agent toggles, Super-Prompt preview,
transparency/cost panes) and wires them to `AppController`.
"""
import streamlit as st
from ragstream.app.controller import AppController

class StreamlitUI:
    """Thin Streamlit wrapper – all logic stays in **AppController**."""
    def __init__(self) -> None:
        self.ctrl = AppController()

    def render(self) -> None:
        """Draws UI components and handles callbacks."""
        st.title("RAG Stream")
        st.write("UI placeholder (Prompt box, ON/OFF file checkboxes, Exact File Lock, agent toggles, Super-Prompt preview)")
PY

################################################################################
echo "[OK] Skeleton files written. You can now open them in your IDE."

```
